{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Deep Learning\\\\RNN\\\\100_Unique_QA_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab\n",
    "vocab = {'<UNK>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    print(row['question'],row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Paris\n",
      "What is the capital of Germany? Berlin\n",
      "Who wrote 'To Kill a Mockingbird'? Harper-Lee\n",
      "What is the largest planet in our solar system? Jupiter\n",
      "What is the boiling point of water in Celsius? 100\n",
      "Who painted the Mona Lisa? Leonardo-da-Vinci\n",
      "What is the square root of 64? 8\n",
      "What is the chemical symbol for gold? Au\n",
      "Which year did World War II end? 1945\n",
      "What is the longest river in the world? Nile\n",
      "What is the capital of Japan? Tokyo\n",
      "Who developed the theory of relativity? Albert-Einstein\n",
      "What is the freezing point of water in Fahrenheit? 32\n",
      "Which planet is known as the Red Planet? Mars\n",
      "Who is the author of '1984'? George-Orwell\n",
      "What is the currency of the United Kingdom? Pound\n",
      "What is the capital of India? Delhi\n",
      "Who discovered gravity? Newton\n",
      "How many continents are there on Earth? 7\n",
      "Which gas do plants use for photosynthesis? CO2\n",
      "What is the smallest prime number? 2\n",
      "Who invented the telephone? Alexander-Graham-Bell\n",
      "What is the capital of Australia? Canberra\n",
      "Which ocean is the largest? Pacific-Ocean\n",
      "What is the speed of light in vacuum? 299,792,458m/s\n",
      "Which language is spoken in Brazil? Portuguese\n",
      "Who discovered penicillin? Alexander-Fleming\n",
      "What is the capital of Canada? Ottawa\n",
      "What is the largest mammal on Earth? Whale\n",
      "Which element has the atomic number 1? Hydrogen\n",
      "What is the tallest mountain in the world? Everest\n",
      "Which city is known as the Big Apple? NewYork\n",
      "How many planets are in the Solar System? 8\n",
      "Who painted 'Starry Night'? vangogh\n",
      "What is the chemical formula of water? H2O\n",
      "What is the capital of Italy? Rome\n",
      "Which country is famous for sushi? Japan\n",
      "Who was the first person to step on the Moon? Armstrong\n",
      "What is the main ingredient in guacamole? Avocado\n",
      "How many sides does a hexagon have? 6\n",
      "What is the currency of China? Yuan\n",
      "Who wrote 'Pride and Prejudice'? Jane-Austen\n",
      "What is the chemical symbol for iron? Fe\n",
      "What is the hardest natural substance on Earth? Diamond\n",
      "Which continent is the largest by area? Asia\n",
      "Who was the first President of the United States? George-Washington\n",
      "Which bird is known for its ability to mimic sounds? Parrot\n",
      "What is the longest-running animated TV show? Simpsons\n",
      "What is the smallest country in the world? VaticanCity\n",
      "Which planet has the most moons? Saturn\n",
      "Who wrote 'Romeo and Juliet'? Shakespeare\n",
      "What is the main gas in Earth's atmosphere? Nitrogen\n",
      "How many bones are in the adult human body? 206\n",
      "Which metal is a liquid at room temperature? Mercury\n",
      "What is the capital of Russia? Moscow\n",
      "Who discovered electricity? Benjamin-Franklin\n",
      "Which is the second-largest country by land area? Canada\n",
      "What is the color of a ripe banana? Yellow\n",
      "Which month has 28 days in a common year? February\n",
      "What is the study of living organisms called? Biology\n",
      "Which country is home to the Great Wall? China\n",
      "What do bees collect from flowers? Nectar\n",
      "What is the opposite of 'day'? Night\n",
      "What is the capital of South Korea? Seoul\n",
      "Who invented the light bulb? Edison\n",
      "Which gas do humans breathe in for survival? Oxygen\n",
      "What is the square root of 144? 12\n",
      "Which country has the pyramids of Giza? Egypt\n",
      "Which sea creature has eight arms? Octopus\n",
      "Which holiday is celebrated on December 25? Christmas\n",
      "What is the currency of Japan? Yen\n",
      "How many legs does a spider have? 8\n",
      "Which sport uses a net, ball, and hoop? Basketball\n",
      "Which country is famous for its kangaroos? Australia\n",
      "Who was the first female Prime Minister of the UK? MargaretThatcher\n",
      "Which is the fastest land animal? Cheetah\n",
      "What is the first element on the periodic table? Hydrogen\n",
      "What is the capital of Spain? Madrid\n",
      "Which planet is the closest to the Sun? Mercury\n",
      "Who is known as the father of computers? CharlesBabbage\n",
      "What is the capital of Mexico? MexicoCity\n",
      "How many colors are in a rainbow? 7\n",
      "Which musical instrument has black and white keys? Piano\n",
      "Who discovered the Americas in 1492? ChristopherColumbus\n",
      "Which Disney character has a long nose and grows it when lying? Pinocchio\n",
      "Who directed the movie 'Titanic'? JamesCameron\n",
      "Which superhero is also known as the Dark Knight? Batman\n",
      "What is the capital of Brazil? Brasilia\n",
      "Which fruit is known as the king of fruits? Mango\n",
      "Which country is known for the Eiffel Tower? France\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(build_vocab,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    tokenized_question =tokenize(row['question'])\n",
    "    tokenized_answer = tokenize(row['answer'])\n",
    "    print(tokenized_question,tokenized_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france'] ['paris']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany'] ['berlin']\n",
      "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird'] ['harper-lee']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system'] ['jupiter']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius'] ['100']\n",
      "['who', 'painted', 'the', 'mona', 'lisa'] ['leonardo-da-vinci']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '64'] ['8']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold'] ['au']\n",
      "['which', 'year', 'did', 'world', 'war', 'ii', 'end'] ['1945']\n",
      "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world'] ['nile']\n",
      "['what', 'is', 'the', 'capital', 'of', 'japan'] ['tokyo']\n",
      "['who', 'developed', 'the', 'theory', 'of', 'relativity'] ['albert-einstein']\n",
      "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit'] ['32']\n",
      "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet'] ['mars']\n",
      "['who', 'is', 'the', 'author', 'of', '1984'] ['george-orwell']\n",
      "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom'] ['pound']\n",
      "['what', 'is', 'the', 'capital', 'of', 'india'] ['delhi']\n",
      "['who', 'discovered', 'gravity'] ['newton']\n",
      "['how', 'many', 'continents', 'are', 'there', 'on', 'earth'] ['7']\n",
      "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis'] ['co2']\n",
      "['what', 'is', 'the', 'smallest', 'prime', 'number'] ['2']\n",
      "['who', 'invented', 'the', 'telephone'] ['alexander-graham-bell']\n",
      "['what', 'is', 'the', 'capital', 'of', 'australia'] ['canberra']\n",
      "['which', 'ocean', 'is', 'the', 'largest'] ['pacific-ocean']\n",
      "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum'] ['299,792,458m/s']\n",
      "['which', 'language', 'is', 'spoken', 'in', 'brazil'] ['portuguese']\n",
      "['who', 'discovered', 'penicillin'] ['alexander-fleming']\n",
      "['what', 'is', 'the', 'capital', 'of', 'canada'] ['ottawa']\n",
      "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth'] ['whale']\n",
      "['which', 'element', 'has', 'the', 'atomic', 'number', '1'] ['hydrogen']\n",
      "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world'] ['everest']\n",
      "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple'] ['newyork']\n",
      "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system'] ['8']\n",
      "['who', 'painted', 'starry', 'night'] ['vangogh']\n",
      "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water'] ['h2o']\n",
      "['what', 'is', 'the', 'capital', 'of', 'italy'] ['rome']\n",
      "['which', 'country', 'is', 'famous', 'for', 'sushi'] ['japan']\n",
      "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon'] ['armstrong']\n",
      "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole'] ['avocado']\n",
      "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have'] ['6']\n",
      "['what', 'is', 'the', 'currency', 'of', 'china'] ['yuan']\n",
      "['who', 'wrote', 'pride', 'and', 'prejudice'] ['jane-austen']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron'] ['fe']\n",
      "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth'] ['diamond']\n",
      "['which', 'continent', 'is', 'the', 'largest', 'by', 'area'] ['asia']\n",
      "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states'] ['george-washington']\n",
      "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds'] ['parrot']\n",
      "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show'] ['simpsons']\n",
      "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world'] ['vaticancity']\n",
      "['which', 'planet', 'has', 'the', 'most', 'moons'] ['saturn']\n",
      "['who', 'wrote', 'romeo', 'and', 'juliet'] ['shakespeare']\n",
      "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere'] ['nitrogen']\n",
      "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body'] ['206']\n",
      "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature'] ['mercury']\n",
      "['what', 'is', 'the', 'capital', 'of', 'russia'] ['moscow']\n",
      "['who', 'discovered', 'electricity'] ['benjamin-franklin']\n",
      "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area'] ['canada']\n",
      "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana'] ['yellow']\n",
      "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year'] ['february']\n",
      "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called'] ['biology']\n",
      "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall'] ['china']\n",
      "['what', 'do', 'bees', 'collect', 'from', 'flowers'] ['nectar']\n",
      "['what', 'is', 'the', 'opposite', 'of', 'day'] ['night']\n",
      "['what', 'is', 'the', 'capital', 'of', 'south', 'korea'] ['seoul']\n",
      "['who', 'invented', 'the', 'light', 'bulb'] ['edison']\n",
      "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival'] ['oxygen']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '144'] ['12']\n",
      "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza'] ['egypt']\n",
      "['which', 'sea', 'creature', 'has', 'eight', 'arms'] ['octopus']\n",
      "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25'] ['christmas']\n",
      "['what', 'is', 'the', 'currency', 'of', 'japan'] ['yen']\n",
      "['how', 'many', 'legs', 'does', 'a', 'spider', 'have'] ['8']\n",
      "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop'] ['basketball']\n",
      "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos'] ['australia']\n",
      "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk'] ['margaretthatcher']\n",
      "['which', 'is', 'the', 'fastest', 'land', 'animal'] ['cheetah']\n",
      "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table'] ['hydrogen']\n",
      "['what', 'is', 'the', 'capital', 'of', 'spain'] ['madrid']\n",
      "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun'] ['mercury']\n",
      "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers'] ['charlesbabbage']\n",
      "['what', 'is', 'the', 'capital', 'of', 'mexico'] ['mexicocity']\n",
      "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow'] ['7']\n",
      "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys'] ['piano']\n",
      "['who', 'discovered', 'the', 'americas', 'in', '1492'] ['christophercolumbus']\n",
      "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying'] ['pinocchio']\n",
      "['who', 'directed', 'the', 'movie', 'titanic'] ['jamescameron']\n",
      "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight'] ['batman']\n",
      "['what', 'is', 'the', 'capital', 'of', 'brazil'] ['brasilia']\n",
      "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits'] ['mango']\n",
      "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower'] ['france']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(build_vocab,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    tokenized_question =tokenize(row['question'])\n",
    "    tokenized_answer = tokenize(row['answer'])\n",
    "    merge_tokens = tokenized_question + tokenized_answer\n",
    "    print(merge_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
      "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
      "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
      "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
      "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
      "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
      "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
      "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
      "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
      "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
      "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
      "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
      "['who', 'discovered', 'gravity', 'newton']\n",
      "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
      "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
      "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
      "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
      "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
      "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
      "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
      "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
      "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
      "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
      "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
      "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
      "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
      "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
      "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
      "['who', 'painted', 'starry', 'night', 'vangogh']\n",
      "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
      "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
      "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
      "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
      "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
      "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
      "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
      "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
      "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
      "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
      "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
      "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
      "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
      "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
      "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
      "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
      "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
      "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
      "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
      "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
      "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
      "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
      "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
      "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
      "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
      "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
      "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
      "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
      "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
      "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
      "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
      "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
      "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
      "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
      "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
      "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
      "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
      "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
      "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
      "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
      "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
      "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
      "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
      "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
      "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
      "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
      "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
      "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
      "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
      "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
      "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
      "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
      "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
      "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(build_vocab,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    tokenized_question = tokenize(row['question'])\n",
    "    tokenized_answer = tokenize(row['answer'])\n",
    "    merge_tokens = tokenized_question + tokenized_answer\n",
    "    for token in merge_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(build_vocab,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert words to Numerical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, vocab):\n",
    "    indexed_text = []  # Initialize the list to hold the indices\n",
    "    for token in tokenize(text):  # Assuming tokenize() splits the text into tokens\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])  # Append the index of the token from vocab\n",
    "        else:                           \n",
    "            indexed_text.append(vocab['<UNK>'])  # Append index for unknown tokens\n",
    "    return indexed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"what is campus\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self,s,vocab):\n",
    "        self.s = s\n",
    "        self.vocab = vocab\n",
    "    def __len__(self):\n",
    "        return self.s.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        numerical_question = text_to_indices(self.s.iloc[index]['question'],self.vocab)\n",
    "        numerical_answer = text_to_indices(self.s.iloc[index]['answer'], self.vocab)\n",
    "        return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(s,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  2,  3,  4,  5, 53]), tensor([54]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,batch_size =1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n"
     ]
    }
   ],
   "source": [
    "for question, answer in dataloader:\n",
    "    print(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim = 50)\n",
    "        self.rnn = nn.RNN(50,64)\n",
    "        self.fc = nn.Linear(64,vocab_size)\n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(324, embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= x(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1833e-01, -1.1170e+00,  5.1767e-01, -1.5383e+00,  1.0005e+00,\n",
       "         -4.2858e-01, -9.9231e-01, -9.6172e-01, -8.4433e-01,  6.5579e-01,\n",
       "          8.1390e-01, -6.9606e-01,  1.1026e+00,  8.1413e-01,  2.7011e-01,\n",
       "         -4.8685e-01,  1.7733e+00, -8.4496e-02,  6.2465e-01,  2.0423e-01,\n",
       "         -1.6315e+00, -1.4993e+00,  2.3353e-01,  6.2691e-01,  4.2534e-01,\n",
       "          2.6960e-01,  3.2549e-01,  2.3054e-01,  2.6680e-01,  6.1313e-03,\n",
       "          5.9163e-01, -4.1707e-01,  4.5404e-03, -2.5233e-01, -1.3228e+00,\n",
       "         -1.0651e-02, -8.7815e-02,  2.7172e-01, -1.7722e+00,  1.8870e+00,\n",
       "          8.0641e-01, -7.1650e-01,  7.1049e-01,  1.1506e+00,  1.0867e+00,\n",
       "         -7.2686e-01, -1.9305e+00, -9.8923e-01,  1.0940e+00, -1.7952e-02],\n",
       "        [ 1.2846e+00,  2.9012e-01, -6.3106e-01,  5.8330e-01, -6.2452e-01,\n",
       "         -1.6767e+00,  9.5702e-01,  5.2430e-01, -9.2684e-01, -1.8426e+00,\n",
       "          1.3264e+00, -2.9998e-01, -4.6919e-01, -2.9637e-01, -3.1309e-01,\n",
       "         -2.7289e-01, -5.8824e-01, -8.0688e-03,  9.9553e-01, -8.8861e-01,\n",
       "         -1.0237e-02,  2.3512e+00, -3.6196e-02,  1.1104e+00, -7.0715e-01,\n",
       "          8.0539e-01, -1.3450e+00,  4.5679e-03, -1.8223e-01,  3.7669e-01,\n",
       "         -4.7102e-01,  1.0798e+00, -1.7022e+00, -1.1315e+00, -2.1910e+00,\n",
       "          2.7104e-02,  5.1813e-03, -1.5720e+00,  3.1692e-01, -3.0781e-01,\n",
       "         -5.3818e-01,  3.6141e-02, -2.5658e-01, -1.8321e+00,  1.5166e+00,\n",
       "         -1.6431e+00,  1.8096e+00, -1.1496e+00, -3.5511e-01, -6.8414e-01],\n",
       "        [ 5.5880e-01, -2.3049e-01, -6.7443e-01,  1.2289e+00, -9.8847e-01,\n",
       "          6.2759e-01, -5.7085e-02,  4.6241e-01, -1.5028e+00, -4.3986e-01,\n",
       "         -1.2703e+00, -1.3753e-01, -4.9444e-01, -2.0806e-03,  7.8382e-01,\n",
       "         -1.5583e+00,  1.2514e+00, -6.8540e-01, -6.2143e-01, -1.2744e+00,\n",
       "          1.0280e-01,  1.6112e+00, -3.5710e-01, -1.7051e+00, -7.3594e-01,\n",
       "         -8.8925e-01,  7.6347e-01,  3.1606e+00, -1.6335e+00,  8.6121e-01,\n",
       "          2.6664e-01,  2.8328e-01, -4.6032e-01,  1.4144e-01,  4.2467e-01,\n",
       "         -6.0395e-01, -1.5261e+00,  9.7110e-01,  7.0100e-01,  4.6516e-01,\n",
       "         -7.1348e-01, -6.6039e-01, -1.3926e+00,  1.3319e+00,  8.9596e-01,\n",
       "         -1.3514e+00, -3.6898e-01,  5.0258e-01, -8.2953e-02,  1.8829e-01],\n",
       "        [-7.3090e-01, -3.1135e-01,  4.2070e-01, -6.2531e-01, -2.2253e+00,\n",
       "         -2.0343e-01,  4.0906e-01,  5.4433e-01, -1.7986e+00,  9.1269e-01,\n",
       "         -1.4569e+00,  8.4417e-01,  1.9153e+00,  1.2294e-01,  3.2289e-01,\n",
       "          1.1802e+00, -8.0817e-01,  1.2776e+00, -2.6429e-01, -2.7444e+00,\n",
       "         -6.7314e-02, -3.7975e-01, -7.7765e-01, -4.9098e-01,  1.9700e+00,\n",
       "          1.0777e+00,  1.1697e-01, -2.2568e-01, -1.4798e+00, -9.8408e-01,\n",
       "         -1.4422e+00, -9.2890e-01,  1.0858e+00,  3.3135e-01,  2.1178e-01,\n",
       "         -7.7334e-01,  9.2801e-02,  4.0489e-01,  1.6660e-01,  1.9218e-01,\n",
       "          5.5656e-01, -1.0850e+00,  4.3466e-01, -1.1812e+00,  7.9377e-01,\n",
       "          8.8672e-01,  1.0606e-01,  8.9251e-01, -1.9583e-01, -1.0774e+00],\n",
       "        [-1.1536e+00, -1.4083e-01, -5.6185e-01,  7.9408e-01, -2.0356e-01,\n",
       "         -4.1633e-01, -6.2345e-01, -4.8819e-01,  8.5990e-01,  2.3510e-01,\n",
       "          1.0723e+00,  1.8138e+00, -1.1286e+00, -1.5387e+00, -9.2334e-01,\n",
       "          1.2138e+00, -7.4477e-01, -1.0895e+00, -9.5142e-01,  7.4990e-02,\n",
       "          3.3848e-02, -4.6843e-01, -2.7190e-01, -8.1989e-01,  1.3899e+00,\n",
       "          5.0068e-01, -1.1498e-01, -6.4986e-01, -1.4454e+00,  3.9474e-01,\n",
       "         -1.2688e+00,  5.4872e-02,  7.5378e-01, -7.9884e-01,  1.5986e+00,\n",
       "         -8.2707e-01, -6.8255e-01,  6.4727e-02,  1.5546e+00,  2.3267e+00,\n",
       "          9.8412e-01,  2.3989e-01, -3.7760e-02,  5.4502e-02,  2.2023e-01,\n",
       "          1.2452e+00,  2.2069e+00, -1.3637e+00, -1.1598e+00,  4.2116e-01],\n",
       "        [ 7.9075e-01, -4.2518e-01, -1.6390e+00, -2.2238e+00,  1.1934e-01,\n",
       "          1.0428e+00,  3.2439e-01,  5.7866e-01, -1.3885e+00,  1.1799e+00,\n",
       "          2.9551e-01, -4.1854e-01,  7.7960e-02, -3.5121e-01, -4.7672e-01,\n",
       "          1.2146e+00, -4.9171e-01,  1.9906e+00, -6.5742e-03, -1.4485e-02,\n",
       "          7.0202e-01, -5.4189e-01, -5.4218e-01, -1.4641e+00, -6.7706e-01,\n",
       "          4.3279e-01, -3.4998e-03,  1.7390e+00, -1.2327e+00, -6.9422e-02,\n",
       "          3.6239e-01, -1.1268e-01,  3.9604e-01,  7.2391e-01, -2.1193e-01,\n",
       "          9.3557e-01,  5.3023e-02,  1.1340e-01, -2.5985e-01, -1.2919e+00,\n",
       "         -2.3747e-01, -3.3083e-01, -1.3480e+00, -3.2678e-01, -6.2308e-01,\n",
       "          1.7941e-01, -2.9589e-01, -8.0512e-01,  1.2326e+00, -1.1744e+00]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nn.RNN(50, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9.5154e-01, -8.1253e-01,  4.7961e-01, -3.9553e-01,  4.3450e-01,\n",
       "           6.0807e-01, -5.4048e-01,  4.4466e-02, -6.8848e-01,  7.0940e-01,\n",
       "           1.7245e-01, -6.6904e-01, -1.0924e-01, -4.5532e-01,  5.3105e-01,\n",
       "           6.9517e-01,  8.5001e-01,  1.0301e-01,  9.3038e-03, -3.5580e-01,\n",
       "          -2.1593e-01, -3.6277e-01, -1.0226e-01,  7.7279e-01, -2.5268e-01,\n",
       "           3.5101e-01, -5.1669e-01,  2.0220e-01, -2.8011e-01,  3.9746e-01,\n",
       "           3.2006e-02,  9.1867e-02,  1.4095e-01,  3.8492e-01, -2.8977e-01,\n",
       "           6.7471e-01,  1.0634e-01, -3.2426e-01, -2.7843e-01,  4.7480e-01,\n",
       "          -3.2695e-01, -2.4871e-01,  6.9683e-01, -3.8063e-01, -1.6154e-01,\n",
       "          -3.2326e-01, -1.1660e-01,  6.5028e-01, -6.5389e-02, -1.0436e-01,\n",
       "           8.3157e-01, -2.9258e-01, -2.5403e-01, -6.9085e-02,  8.2393e-02,\n",
       "           1.8684e-01, -2.5019e-01,  8.1364e-02,  2.8573e-02,  2.7748e-01,\n",
       "           1.6973e-01, -2.2118e-01,  3.0429e-01,  4.5155e-01],\n",
       "         [-4.1956e-01,  2.1453e-01,  5.0081e-01, -5.8735e-01,  3.2884e-01,\n",
       "           7.5164e-02,  3.3785e-01, -4.3983e-01, -5.3298e-01, -4.9616e-02,\n",
       "          -3.5576e-01,  3.1496e-01, -7.5055e-01, -6.3303e-01,  3.3074e-01,\n",
       "           5.0608e-01,  8.0905e-02,  3.3995e-01,  3.5426e-01,  5.7059e-01,\n",
       "          -5.7122e-01, -1.0504e-01, -5.0314e-01,  8.5008e-02,  7.0304e-02,\n",
       "          -4.5094e-01,  6.8231e-01,  5.0068e-01, -1.1281e-01,  3.5882e-01,\n",
       "           3.3351e-01,  4.6980e-01,  8.5238e-01, -7.3230e-02,  5.5029e-01,\n",
       "          -4.7114e-01, -3.9427e-01,  7.1332e-01,  2.6105e-01, -4.7016e-01,\n",
       "           5.3106e-01,  3.3337e-01,  1.1830e-01,  4.5500e-01,  3.3620e-01,\n",
       "          -2.1183e-01, -2.5586e-01, -8.4069e-01,  5.3040e-01, -6.4259e-01,\n",
       "          -7.1153e-01,  5.7855e-01,  6.8555e-01, -9.2670e-03,  1.6973e-01,\n",
       "           1.9370e-01, -2.1645e-01,  2.2807e-01, -2.1075e-01, -4.0051e-02,\n",
       "           2.6723e-02, -8.0513e-01, -5.9888e-01,  1.4399e-01],\n",
       "         [-7.3429e-01,  4.7663e-01, -4.5751e-01,  2.4337e-01,  4.5568e-01,\n",
       "          -7.2911e-01,  3.3302e-01,  6.7377e-03, -6.1807e-01,  2.9847e-01,\n",
       "           5.0726e-01,  6.6260e-01,  1.0276e-01,  6.3779e-01,  1.6732e-03,\n",
       "          -3.0805e-01, -7.1514e-01,  1.5034e-01, -2.2892e-01,  1.3129e-01,\n",
       "          -4.5533e-01, -2.6244e-01,  6.8282e-01,  2.2588e-01,  3.3268e-01,\n",
       "          -7.1283e-01,  6.8446e-02,  2.3835e-01,  7.5530e-01,  4.5082e-01,\n",
       "           9.2392e-02,  8.2186e-01,  8.2807e-01, -3.2576e-01, -2.9079e-01,\n",
       "          -3.3565e-01, -3.8793e-01, -4.5541e-02,  5.1780e-01,  4.4012e-01,\n",
       "          -2.4100e-01, -2.1460e-01, -9.8978e-02,  1.5355e-02,  8.7350e-02,\n",
       "           6.5363e-01,  4.1014e-01, -5.4340e-02,  3.0888e-01, -6.4159e-01,\n",
       "          -7.9196e-02,  4.2462e-01,  2.2280e-01,  8.6936e-02, -5.6068e-01,\n",
       "          -1.6434e-01,  4.5881e-01,  6.0752e-01, -6.8401e-01,  3.9183e-01,\n",
       "          -2.8737e-02,  1.6964e-02,  5.9543e-01, -4.0463e-01],\n",
       "         [-3.4254e-01,  5.0443e-01,  3.4080e-01,  5.2073e-01,  1.2130e-02,\n",
       "          -1.6617e-01,  1.4633e-01,  2.7865e-01, -1.7521e-02,  3.9580e-01,\n",
       "           5.0698e-01,  5.6671e-01, -3.5813e-01, -1.5232e-01, -6.4919e-01,\n",
       "           5.1084e-01,  2.0552e-01, -2.6693e-01,  1.5900e-03,  3.9715e-01,\n",
       "           8.3400e-01, -2.5906e-01,  8.2275e-01,  2.7855e-02,  4.7762e-02,\n",
       "           3.8508e-01, -2.5660e-01, -7.7388e-01,  4.9894e-01, -2.9063e-01,\n",
       "           2.2609e-01, -4.8067e-02, -2.3412e-01,  3.1391e-01, -3.6063e-01,\n",
       "           6.0864e-01,  4.4233e-01,  5.1741e-02,  9.7753e-02,  6.8889e-01,\n",
       "           2.6336e-01, -5.7233e-01,  3.4343e-01, -7.7544e-01, -1.0530e-01,\n",
       "           3.5531e-01,  2.0990e-02, -9.1952e-02, -2.5632e-01, -3.0483e-01,\n",
       "          -6.8657e-01,  5.2972e-01,  1.3810e-01,  6.1120e-01,  4.5265e-01,\n",
       "          -1.5372e-01, -4.4162e-01, -3.9631e-01,  9.3768e-01,  6.9115e-01,\n",
       "          -2.3783e-01, -3.5154e-01, -3.4597e-01,  3.8584e-01],\n",
       "         [-3.2195e-01,  6.6719e-01, -1.6782e-01,  1.8131e-01, -4.6646e-01,\n",
       "           5.9277e-01, -1.5577e-02, -2.9182e-01,  7.2297e-01, -3.6691e-01,\n",
       "          -8.1942e-01, -1.8028e-01, -6.6339e-02, -1.7777e-01,  3.9101e-01,\n",
       "           7.1695e-02, -4.9032e-01, -8.2207e-01,  4.6274e-01,  1.0679e-01,\n",
       "           6.7398e-01,  7.2774e-02,  3.8909e-01,  5.6970e-01,  1.6338e-02,\n",
       "           4.4161e-01,  7.1323e-01,  1.8658e-01,  5.1399e-01, -2.3135e-01,\n",
       "          -3.3223e-01,  1.2992e-02,  8.9425e-01, -3.4860e-01,  5.8875e-01,\n",
       "           1.9149e-01,  4.7256e-01, -5.5680e-02,  4.1679e-01,  1.1664e-01,\n",
       "           8.5209e-04,  4.9355e-01, -4.9856e-01,  7.3394e-01, -6.8977e-01,\n",
       "          -7.7665e-01, -1.0154e-01,  2.8880e-01, -5.7810e-01,  6.4763e-01,\n",
       "           1.5192e-01, -1.5063e-01,  5.6459e-01,  8.7891e-01, -3.0139e-01,\n",
       "          -4.2053e-01, -1.5044e-01, -2.0804e-01, -9.6959e-02, -8.6152e-01,\n",
       "          -4.4526e-01,  3.4772e-01, -5.9320e-01,  2.0978e-01],\n",
       "         [-4.4058e-01,  7.0092e-01, -1.6598e-01, -7.7533e-02,  1.3106e-01,\n",
       "          -4.3066e-01, -7.7176e-01, -3.5798e-01,  2.4479e-01, -8.0555e-01,\n",
       "          -2.4340e-01,  7.0933e-02, -2.8633e-01, -1.1425e-01,  9.6193e-02,\n",
       "           4.8880e-01, -5.1119e-01,  7.2911e-01,  3.5278e-01, -3.1918e-02,\n",
       "          -3.2690e-01, -6.9946e-02,  2.3430e-01,  3.6317e-01, -5.4178e-01,\n",
       "           5.3716e-02, -2.9733e-01, -3.6589e-01,  3.7475e-01, -8.0979e-01,\n",
       "           5.9703e-01, -3.3467e-01, -4.4446e-01, -7.2863e-01, -4.2814e-01,\n",
       "           6.1703e-02, -8.0306e-03,  7.0518e-01, -4.7798e-01,  3.8821e-01,\n",
       "          -2.0664e-01, -5.3387e-01,  8.0409e-01, -1.8987e-02,  1.1787e-03,\n",
       "          -1.0141e-01, -2.7101e-02, -1.0206e-02, -1.3718e-01, -4.0128e-01,\n",
       "          -3.8178e-01, -8.2855e-01, -1.8572e-01, -4.4335e-01,  2.3037e-01,\n",
       "          -4.7581e-01,  6.4158e-01, -3.8913e-01,  5.8095e-01,  3.9360e-01,\n",
       "           2.3000e-01,  4.8005e-01,  2.9118e-01,  1.1904e-01]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.4406,  0.7009, -0.1660, -0.0775,  0.1311, -0.4307, -0.7718, -0.3580,\n",
       "           0.2448, -0.8055, -0.2434,  0.0709, -0.2863, -0.1143,  0.0962,  0.4888,\n",
       "          -0.5112,  0.7291,  0.3528, -0.0319, -0.3269, -0.0699,  0.2343,  0.3632,\n",
       "          -0.5418,  0.0537, -0.2973, -0.3659,  0.3747, -0.8098,  0.5970, -0.3347,\n",
       "          -0.4445, -0.7286, -0.4281,  0.0617, -0.0080,  0.7052, -0.4780,  0.3882,\n",
       "          -0.2066, -0.5339,  0.8041, -0.0190,  0.0012, -0.1014, -0.0271, -0.0102,\n",
       "          -0.1372, -0.4013, -0.3818, -0.8285, -0.1857, -0.4434,  0.2304, -0.4758,\n",
       "           0.6416, -0.3891,  0.5809,  0.3936,  0.2300,  0.4801,  0.2912,  0.1190]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = y(a)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.Linear(64, 324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2052, -0.1455,  0.1638, -0.0133,  0.0981,  0.0259,  0.0522,  0.1711,\n",
       "         -0.0106, -0.2091,  0.2101,  0.0675,  0.1206,  0.1219, -0.4490,  0.3261,\n",
       "          0.0786,  0.2257,  0.0058,  0.3043, -0.2789,  0.2326,  0.4892,  0.1542,\n",
       "         -0.3761, -0.3266,  0.3313,  0.0300, -0.3474,  0.2813, -0.0066, -0.2051,\n",
       "          0.0834,  0.3263,  0.2266, -0.1112, -0.1066,  0.1209,  0.1014, -0.1051,\n",
       "         -0.0203,  0.1400, -0.0453,  0.2297,  0.3249,  0.0804,  0.1707, -0.0580,\n",
       "          0.1312,  0.2165, -0.0855, -0.3389,  0.1973, -0.3779,  0.2006,  0.1864,\n",
       "          0.5404, -0.0207, -0.5332,  0.0525, -0.1684,  0.0136, -0.5946,  0.2151,\n",
       "          0.5137, -0.1968, -0.1171, -0.1600, -0.0683,  0.1860, -0.1112,  0.0508,\n",
       "          0.4815,  0.4075,  0.2942, -0.2614,  0.0084, -0.1190,  0.0583,  0.4337,\n",
       "         -0.3256,  0.0472, -0.2740,  0.1413,  0.0064,  0.2316,  0.2348,  0.0714,\n",
       "          0.3914, -0.2839,  0.1058, -0.0274, -0.2113, -0.1589, -0.1475, -0.2991,\n",
       "          0.3876, -0.3157,  0.2709, -0.3154,  0.0027,  0.6019, -0.3736, -0.1244,\n",
       "          0.1129, -0.0790,  0.0199,  0.1195,  0.0084,  0.0500, -0.3425, -0.0546,\n",
       "         -0.4644,  0.0622,  0.2783, -0.0819,  0.0536, -0.0189,  0.0330, -0.1979,\n",
       "          0.4186,  0.2965, -0.0580,  0.2910, -0.4079, -0.3582,  0.3248, -0.3886,\n",
       "         -0.1267, -0.3433,  0.0475, -0.0544, -0.2694, -0.4834, -0.3397, -0.1607,\n",
       "         -0.2150,  0.0202, -0.0815,  0.4427,  0.0671,  0.1005,  0.1733, -0.2641,\n",
       "         -0.0220,  0.0575, -0.1014, -0.2043, -0.3240, -0.3081, -0.8102, -0.1648,\n",
       "          0.2314,  0.3848,  0.2133, -0.3675, -0.0021,  0.1015, -0.1178,  0.4835,\n",
       "          0.4046,  0.2081,  0.2970,  0.0243,  0.1443,  0.2735, -0.0336, -0.1991,\n",
       "          0.4585,  0.0059,  0.1474,  0.0786, -0.1809, -0.0457, -0.0426,  0.0624,\n",
       "         -0.1845, -0.0209,  0.2220,  0.1796, -0.2162,  0.0041, -0.0453, -0.0488,\n",
       "          0.0028, -0.1673, -0.1477,  0.3171,  0.1268,  0.1378,  0.2112, -0.4374,\n",
       "          0.1776,  0.2296,  0.0408,  0.0028, -0.1737,  0.1523, -0.3985, -0.0402,\n",
       "         -0.1741,  0.0395,  0.2537,  0.4720,  0.1755, -0.0983, -0.4164,  0.2651,\n",
       "         -0.1001, -0.3218,  0.0607,  0.1090, -0.0857,  0.2563,  0.5417, -0.2079,\n",
       "          0.1144,  0.1108, -0.2401, -0.4142, -0.0172, -0.1137,  0.0747, -0.4185,\n",
       "         -0.1237,  0.1324, -0.2044,  0.3103,  0.1463,  0.0266, -0.0355,  0.1071,\n",
       "          0.1161, -0.2121, -0.1421,  0.2572, -0.1986,  0.1277, -0.1727,  0.2689,\n",
       "          0.2105, -0.0323,  0.3007,  0.6400,  0.2044,  0.0318,  0.2542,  0.0044,\n",
       "          0.3925,  0.2720, -0.2318, -0.4253,  0.3023, -0.0720,  0.0076, -0.4579,\n",
       "         -0.4320, -0.5369, -0.3602, -0.1559, -0.3325, -0.2500,  0.1408, -0.1976,\n",
       "         -0.0044, -0.0541, -0.1658, -0.2299,  0.2598, -0.2188,  0.5528, -0.1049,\n",
       "         -0.3192,  0.1039, -0.1556,  0.2877,  0.1412,  0.2149,  0.2666,  0.0222,\n",
       "          0.0486, -0.3649,  0.0081, -0.2964, -0.2545,  0.0672, -0.0119,  0.5076,\n",
       "         -0.1439,  0.1950,  0.1605,  0.3469,  0.5568,  0.4138,  0.7487,  0.0201,\n",
       "          0.1394,  0.3224, -0.3011,  0.0899,  0.0838,  0.3587,  0.2543,  0.4085,\n",
       "         -0.1151, -0.1058,  0.3003,  0.3244,  0.2882,  0.2559,  0.0585,  0.1624,\n",
       "         -0.2181,  0.0025,  0.1175,  0.3813,  0.0098, -0.2580,  0.1472, -0.2406,\n",
       "         -0.2509,  0.2186,  0.2613, -0.5087]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 324])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim = 50)\n",
    "        self.rnn = nn.RNN(50,64, batch_first = True)\n",
    "        self.fc = nn.Linear(64,vocab_size)\n",
    "    def forward(self, question):\n",
    "        embedded_question = self.embedding(question)\n",
    "        hidden, final = self.rnn(embedded_question)\n",
    "        output = self.fc(final.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 524.7996\n",
      "Epoch: 2, Loss: 456.6656\n",
      "Epoch: 3, Loss: 378.0373\n",
      "Epoch: 4, Loss: 316.9381\n",
      "Epoch: 5, Loss: 266.7683\n",
      "Epoch: 6, Loss: 219.1716\n",
      "Epoch: 7, Loss: 174.6392\n",
      "Epoch: 8, Loss: 135.9777\n",
      "Epoch: 9, Loss: 104.7241\n",
      "Epoch: 10, Loss: 79.8385\n",
      "Epoch: 11, Loss: 61.1985\n",
      "Epoch: 12, Loss: 47.3499\n",
      "Epoch: 13, Loss: 37.2820\n",
      "Epoch: 14, Loss: 29.5850\n",
      "Epoch: 15, Loss: 24.1715\n",
      "Epoch: 16, Loss: 20.1306\n",
      "Epoch: 17, Loss: 16.7297\n",
      "Epoch: 18, Loss: 14.2197\n",
      "Epoch: 19, Loss: 12.1865\n",
      "Epoch: 20, Loss: 10.4895\n",
      "Epoch: 21, Loss: 9.1591\n",
      "Epoch: 22, Loss: 8.0676\n",
      "Epoch: 23, Loss: 7.1514\n",
      "Epoch: 24, Loss: 6.3927\n",
      "Epoch: 25, Loss: 5.7152\n",
      "Epoch: 26, Loss: 5.1711\n",
      "Epoch: 27, Loss: 4.6784\n",
      "Epoch: 28, Loss: 4.2397\n",
      "Epoch: 29, Loss: 3.8754\n",
      "Epoch: 30, Loss: 3.5522\n",
      "Epoch: 31, Loss: 3.2586\n",
      "Epoch: 32, Loss: 2.9988\n",
      "Epoch: 33, Loss: 2.7754\n",
      "Epoch: 34, Loss: 2.5659\n",
      "Epoch: 35, Loss: 2.3802\n",
      "Epoch: 36, Loss: 2.2071\n",
      "Epoch: 37, Loss: 2.0561\n",
      "Epoch: 38, Loss: 1.9164\n",
      "Epoch: 39, Loss: 1.7925\n",
      "Epoch: 40, Loss: 1.6724\n",
      "Epoch: 41, Loss: 1.5666\n",
      "Epoch: 42, Loss: 1.4648\n",
      "Epoch: 43, Loss: 1.3766\n",
      "Epoch: 44, Loss: 1.2915\n",
      "Epoch: 45, Loss: 1.2152\n",
      "Epoch: 46, Loss: 1.1407\n",
      "Epoch: 47, Loss: 1.0745\n",
      "Epoch: 48, Loss: 1.0126\n",
      "Epoch: 49, Loss: 0.9540\n",
      "Epoch: 50, Loss: 0.9016\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for question, answer in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(question)\n",
    "        # loss calculation\n",
    "        loss = criterion(output, answer[0])  \n",
    "        # gradients\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.argmax(output, dim=1)  # Get the index of the highest probability\n",
    "        if predicted.item() == answer[0].item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold = 0.5):\n",
    "    # convert question to numbers\n",
    "    numerical_question =  text_to_indices(question,vocab)\n",
    "    # tensor\n",
    "    question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "    # sent to model\n",
    "    output = model(question_tensor)\n",
    "    # convert logits to probabilits\n",
    "    probs = torch.nn.functional.softmax(output, dim =1)\n",
    "    # find index of max prob\n",
    "    value, index = torch.max(probs, dim = 1)\n",
    "    print(value,index)\n",
    "    if value<threshold:\n",
    "        print(\"I dont know\")\n",
    "    else:\n",
    "        \n",
    "        print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9808], grad_fn=<MaxBackward0>) tensor([54])\n",
      "tokyo\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"what is the capital of japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
